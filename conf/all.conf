#======================CEPH DEPLOY=======================
#ceph config path
conf_path="/etc/ceph/ceph.conf"
# username for all host
deploy_username=ceph
# ceph stable version
deploy_ceph_version=hammer
# monitor servers , splited by ','
deploy_mon_servers=ssdosd5
# osds servers, splited by ','
deploy_osd_servers=ssdosd5,ssdosd6
# mds servers, splited by ','
#deploy_mds_servers=aceph01
# rbd clients, split by ','
deploy_rbd_nodes=ceph-client1,ceph-client2
# osd-journal list , splited by ','
#aceph01=/dev/sda1:/dev/sdb1,/dev/sdd1:/dev/sdb2,/dev/sde1:/dev/sdb3,/dev/sdf1:/dev/sdb4,/dev/sdg1:/dev/sdb5,/dev/sdh1:/dev/sdc1,/dev/sdj1:/dev/sdc2,/dev/sdk1:/dev/sdc3,/dev/sdl1:/dev/sdc4,/dev/sdm1:/dev/sdc5
#aceph02=/dev/sdc1:/dev/sda1,/dev/sdd1:/dev/sda2,/dev/sde1:/dev/sda3,/dev/sdf1:/dev/sda4,/dev/sdg1:/dev/sda5,/dev/sdh1:/dev/sdb1,/dev/sdj1:/dev/sdb2,/dev/sdk1:/dev/sdb3,/dev/sdl1:/dev/sdb4,/dev/sdm1:/dev/sdb5
#aceph03=/dev/sdc1:/dev/sda1,/dev/sdd1:/dev/sda2,/dev/sde1:/dev/sda3,/dev/sdf1:/dev/sda4,/dev/sdg1:/dev/sda5,/dev/sdh1:/dev/sdb1,/dev/sdj1:/dev/sdb2,/dev/sdk1:/dev/sdb3,/dev/sdl1:/dev/sdb4,/dev/sdm1:/dev/sdb5
#aceph04=/dev/sdc1:/dev/sda1,/dev/sdd1:/dev/sda2,/dev/sde1:/dev/sda3,/dev/sdf1:/dev/sda4,/dev/sdg1:/dev/sda5,/dev/sdh1:/dev/sdb1,/dev/sdj1:/dev/sdb2,/dev/sdk1:/dev/sdb3,/dev/sdl1:/dev/sdb4,/dev/sdm1:/dev/sdb5
#aceph01=/dev/sdc1:/dev/sdi3,/dev/sde1:/dev/sdi4
ssdosd5=/dev/sda1:/dev/sdn1,/dev/sdb1:/dev/sdn2,/dev/sdc1:/dev/sdn3,/dev/sdd1:/dev/sdn4,/dev/sde1:/dev/sdo1,/dev/sdf1:/dev/sdo2,/dev/sdg1:/dev/sdo3,/dev/sdh1:/dev/sdo4,/dev/sdi1:/dev/sdp1,/dev/sdj1:/dev/sdp2,/dev/sdk1:/dev/sdp3,/dev/sdl1:/dev/sdp4
ssdosd6=/dev/sde1:/dev/sda1,/dev/sdf1:/dev/sda2,/dev/sdg1:/dev/sda3,/dev/sdh1:/dev/sda4,/dev/sdi1:/dev/sdc1,/dev/sdj1:/dev/sdc2,/dev/sdk1:/dev/sdc3,/dev/sdl1:/dev/sdc4,/dev/sdm1:/dev/sdd1,/dev/sdn1:/dev/sdd2,/dev/sdo1:/dev/sdd3,/dev/sdp1:/dev/sdd4
# osd partition_count on one HDD, and the size( default will use the full disk )
osd_partition_count=1
osd_partition_size=200G
# journal partition_count on one HDD, and the size
journal_partition_count=4
journal_partition_size=10G
#
#=====================TEST DEPLOY=====================
list_vclient=vclient01,vclient02,vclient03,vclient04,vclient05,vclient06,vclient07,vclient08,vclient09,vclient10,vclient11,vclient12,vclient13,vclient14,vclient15,vclient16,vclient17,vclient18,vclient19,vclient20,vclient21,vclient22,vclient23,vclient24,vclient25,vclient26,vclient27,vclient28,vclient29,vclient30,vclient31,vclient32,vclient33,vclient34,vclient35,vclient36,vclient37,vclient38,vclient39,vclient40,vclient41,vclient42,vclient43,vclient44,vclient45,vclient46,vclient47,vclient48,vclient49,vclient50,vclient51,vclient52,vclient53,vclient54,vclient55,vclient56,vclient57,vclient58,vclient59,vclient60
head=ceph-client1
tmp_dir=/opt
fio_for_libcephfs_dir=/opt
user=root
list_mon=ssdosd5
list_client=ceph-client1,ceph-client2
list_ceph=ssdosd5,ssdosd6
list_nic=ssdosd5:eth1,ssdosd6:p801p1
volume_size=10240
# the cpu vm cpupin start with
cpuset_start=0
# the max num of vms in each hypervisor/node 
vm_num_per_client=30
# img_path_dir
img_path_dir=/mnt/images
# ip_prefix
ip_prefix=192.168.9
# ip_fix_start , vm will be created from ip_prefix.if_fix, example: if set ip_prefix = 192.168.9; ip_fix = 201, then the first vm ip will be 192.168.9.201, the second vm should be 192.168.9.202 and so on
ip_fix=101
vm_image_locate_server=10.239.158.45
#
#=====================TEST PART====================
rbd_volume_count=60
# vm # , split by ','
run_vm_num=60
# disk , split by ','
run_file=/dev/vdb
# test size , split by ','
run_size=10g
# io pattern, split by ','
run_io_pattern=randwrite
# record size, split by ','
run_record_size=4k
# queue depth, split by ','
run_queue_depth=8
# warm-up time
run_warmup_time=100
# run time
run_time=300
# destination directory
dest_dir=/mnt/data/
# destination directory
dest_dir_remote_bak=192.168.3.101:/data4/xiaojuan/flashcache_24osd
# only used in fio_rbd engine test scenario
rbd_num_per_client=30,30
# specify the distribution of fio workload
fio_zipf=True
#====================CEPH CONFIGURATION===================
[ceph_conf]
public_networ=tk = 10.10.8.0/24
cluster_network = 10.10.8.0/24
osd_pool_default_pg_num = 1024
osd_pool_default_pgp_num = 1024
