[help]
user = only support root currently
head = CeTune Controller Node hostname, the node should be able to auto ssh to all CeTune workers and connect to ceph monitor
list_server = List osd nodes here, split by ','; Ex: ${hostname1},${hostname2},...
list_client = List Client(rbd/cosbench) nodes, split by ','; Ex: ${hostname1},${hostname2},...
list_mon = List monitor nodes, split by ','; Ex: ${hostname1},${hostname2},...
list_vclient = List vm nodes, split by ','; Ex: ${hostname1},${hostname2},...
rgw_server = List rgw nodes, split by ','; Ex: ${hostname1},${hostname2},...
public_network = network connected with client(rbd, rgw) and OSD nodes; Ex: 10.10.5.0/24
cluster_network = network connected between OSD nodes; Ex: 10.10.5.0/24
monitor_network = network connected between monitor to OSD nodes and CeTune Head; Ex: 10.10.5.0/24
osd_objectstore = filestore/bluestore/keyvaluestore, set to filestore by default
version = hammer/infernalis/jewel...
disk|read_ahead_kb = system disk read_ahead_kb, set to 2048 by default.
pool|rbd|size = rbd replica size, set to 2 by default
analyzer = what data will be analyzed at analyzing phase(not finished), set to all by default.
enable_rgw = true/false; if or if not enable rgw
rgw_num_per_server = How many rgw daemon will be run on one node, set to 5 by default
rgw_start_index = set to 1 by default, most cases no need to change
cosbench_auth_username = set to 'cosbench:operator' by default, no need to change unless you change the cosbench auth username
cosbench_auth_password = set to 'intel2012' by default, no need to change unless you change the cosbench auth passwd
cosbench_controller_proxy = set to "" by default, if proxy is required to connect to cosbench controller, please add it here
clean_build = true/false; set to true by default; if set to false, deploy ceph cluster will not clean current setup and try to only deploy new added osd/rgw to ceph cluster.
collector = set to "blktrace,strace,fatrace,lttng,perfcounter" by default; Please make sure the collector is installed, or just remove the collector name from here; blktrace will hurt performance a bit, fatrace is only support under ubuntu.
dest_dir_remote_bak = Ex: 192.168.3.101:/share/ceph_data/
monitoring_interval = set to 1 by default; if the test runtime is quite long like 3600 sec, can just increase the interval here, so it reduces the space occupation.
perfcounter_data_type = librbd/osd/filestore/object...; If the data can be collected by admin_socket perf dump, you can add the data type here
perfcounter_time_precision_level = set to 6 by default, so time precision is usec.
rbd_volume_count = How many rbd volume should be create in total to complete the benchmark test.
volume_size = The rbd volume size should be used when creating rbd volume.
fio_capping = true/false; if true, Per fio job's sequential Bandwidth will be throttled to 60MB/s, random iops will be throttled to 100.
enable_zipf = true/false; if true, fio offset layout will follow zipf's law.
fio_randrepeat = true/false; Seed the random number generator in a predictable way so results are repeatable across runs. Default: true. 
rwmixread = fio read write mix ratio, if set to 100 indicates to a 100% pure read. if set to 80 indicates to a read:20, write:80 mix readwrite.
tmp_dir = set to '/opt' by default, The place to store data temporary, this dir should have enough space on all CeTune workers nodes.
dest_dir = After benchmarkdown, this directory is used to store CeTune report data.
disk_num_per_client = How many rbd volume will be attached to one hypervisor.
cache_drop_level = set to 3 by default, will drop inode and pagecache data; set to 1 is only drop pagecache data.
cosbench_folder = set to /opt/cosbench by default, cosbench codes locate there and CeTune will start cosbench daemon there.
cosbench_config_dir = set to /opt/cosbench/config by default, CeTune will create cosbench benchmark xml there
cosbench_driver = hosts run cosbench drivers/workers on.
cosbench_admin_ip = cosbench controller ip
cosbench_cluster_ip = RGW node ip
cosbench_network = network connected by cosbench controller and workers. Ex: 10.10.5.0/24
cosbench_controller = A host runs cosbench controller on, this node should be able to connected by CeTune and ceph rgw nodes.
cosbench_version = set to v0.4.c2 by default, when run a cosbench benchmark test, this cosbench version will be retained from github
custom_script = if benchmark_driver set to 'hook', user can add a '*.bash/*.sh' here, and CeTune will run this script at runtime. Ex: "bash hook.bash"
disk_format = disk type and sequence of ${hostname}.
disable_tuning_check = if true, cetune will not check ceph.conf before benchmark.
distributed_data_process = if true, cetune will distribute the data process work to all nodes and summarize in controller node. This option requires running controller_dependencies_install.py on all cetune workers.
